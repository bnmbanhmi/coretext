# CoreText Evaluation Methodology
**Version:** 1.0
**Date:** 2026-01-27
**Objective:** Comparative analysis of Graph-Based Context Retrieval (CoreText) vs. File-Based Retrieval in AI-driven software generation.

## 1. Experimental Overview
This study evaluates whether CoreText's knowledge-graph retrieval method reduces token consumption and latency while maintaining code quality compared to standard file-based context retrieval methods.

### 1.1 The Task (Workload)
**Subject:** Rental House Listing Platform.
**Complexity Requirements:**
To ensure sufficient complexity for valid differentiation, the specification must include:
- **User Roles:** Multi-tenant architecture (Admin, Landlord, Tenant).
- **Data Relationships:** Complex 1:N and N:N relationships (Listings, Reviews, Bookings, Users).
- **Features:** Search/Filtering, Auth/RBAC, Booking State Machine.
- **Volume:** Specification documents (PRD, Architecture, User Stories) must exceed 10,000 tokens total to stress-test context windows.

---

## 2. Experimental Subjects

### Subject A: Human Baseline (Control 1)
- **Operator:** Senior Developer (Human).
- **Tools:** Standard IDE + AI Coding Assistant (e.g., Copilot/Chat interface).
- **Constraint:** Operator builds the software manually based on BMM specs.
- **Goal:** Establish the "Gold Standard" for logic and execution time.

### Subject B: Standard BMad Agent (Control 2)
- **Agent:** Standard `bmad-developer` agent.
- **Retrieval Method:** **File-Based**. Agent reads full Markdown files (PRD.md, Architecture.md) into context.
- **Workflow:** Standard ReAct loop (Read File -> Plan -> Write Code).
- **Constraint:** Human intervention limited to "Orchestration Only" (running commands, providing error logs).

### Subject C: CoreText Integrated Agent (Test)
- **Agent:** Modified `bmad-developer` agent.
- **Retrieval Method:** **Graph-Based**. Agent uses `coretext-retrieve` (or `query_knowledge`) tool to query specific subgraphs relevant to the current task.
- **Workflow:** Graph Query -> Bundle Context -> Write Code.
- **Constraint:** **Strict Knowledge Isolation**. The agent must NOT have access to the raw Markdown specification files; it must rely solely on the Knowledge Graph.

---

## 3. Metrics & Measurement

### 3.1 Efficiency Metrics (Quantitative)
| Metric | Unit | Definition |
| :--- | :--- | :--- |
| **Input Tokens** | Count | Total tokens sent TO the LLM (Context). **Primary Metric.** |
| **Output Tokens** | Count | Total tokens generated BY the LLM (Code/Reasoning). |
| **API Cost** | USD | Calculated cost based on model pricing. |
| **Context Utility**| Ratio | `(Lines of Code Generated) / (Input Tokens)`. Higher is better. |

### 3.2 Performance Metrics (Temporal)
| Metric | Unit | Definition |
| :--- | :--- | :--- |
| **Total Wall Time** | Minutes | Start to Finish (Definition of Done). |
| **AI Compute Time**| Minutes | Time spent waiting for LLM responses. |
| **Human Action Time**| Minutes| Time spent by human reading, thinking, or typing. |
| **Intervention Count**| Count | Number of times the AI got stuck and needed human guidance. |

### 3.3 Quality Scorecard (0-100 Scale)
Post-build evaluation of the final artifact.
1. **Functional Completeness (40 pts):** % of User Stories in Spec marked "Verified".
2. **Code Health (20 pts):** `100 - (Linter Errors + Warnings)` using a pre-defined config (e.g., Ruff/ESLint).
3. **Test Coverage (20 pts):** % of lines covered by generated tests.
4. **Bug Resilience (20 pts):** Result of 10-minute "Monkey Test" (random inputs).

---

## 4. Execution Protocol

### Phase 1: Specification Generation (Frozen)
1. Run `bmm` (Business Model Module) to generate the Rental Platform Plan.
2. **FREEZE** the output files (PRD, Tech Spec, Stories).
3. These exact files are the input for ALL three experiments.

### Phase 2: Experiment Execution (Run 3 Times)
**Pre-Flight Checklist (The "Clean Room" Rule):**
- [ ] Wipe target directory.
- [ ] Clear LLM conversation history.
- [ ] Reset Database/Docker containers.
- [ ] Verify Model Version (e.g., `gpt-4o-2024-05-13`) is identical for all runs.
- [ ] **Subject C Only:** Ingest Phase 1 specs into CoreText DB and disable `read_file` for spec path.

**Operator Rules of Engagement:**
- **For Subject A (Human):** Code naturally.
- **For Subject B & C (AI):** Do NOT write code. Only execute commands requested by the agent. If an error occurs, paste the error exactly. Log every intervention.

### Phase 3: Analysis
- Compare **Subject C** vs. **Subject B** to validate the efficiency of Graph Retrieval.
- Compare both against **Subject A** to assess the "Reality Gap" between agentic and human development.

## 5. Operational Setup

### 5.1 Repository & Worktree Strategy
The experiment utilizes **Git Worktrees** to maintain parallel, isolated workspaces for each subject. This allows for side-by-side comparison and prevents dependency/environment pollution between runs.

**Worktree Structure:**
Experimental worktrees are created as **sibling directories** under a common parent (e.g., `~/Git/`) to ensure absolute isolation and clear path management.

1. **`~/Git/coretext`**: The main repository (`main` branch).
2. **`~/Git/coretext-exp-01-human`**: Checked out to `experiment/baseline-human`.
3. **`~/Git/coretext-exp-02-std`**: Checked out to `experiment/subject-b-standard`.
4. **`~/Git/coretext-exp-03-ctx`**: Checked out to `experiment/subject-c-coretext`.

**Implementation Commands:**
```bash
cd ~/Git/coretext

# Setup branches
git branch experiment/baseline-human main
git branch experiment/subject-b-standard main
git branch experiment/subject-c-coretext main

# Create Worktrees as siblings
git worktree add ../coretext-exp-01-human experiment/baseline-human
git worktree add ../coretext-exp-02-std experiment/subject-b-standard
git worktree add ../coretext-exp-03-ctx experiment/subject-c-coretext
```

**Maintenance:**
- **Bugfixes:** If a bug is fixed in `main`, navigate to each sibling directory (e.g., `cd ../coretext-exp-03-ctx`) and run `git merge main`.
- **Comparison:** Open each directory as a separate window in your IDE or use `diff -r ../coretext-exp-02-std ../coretext-exp-03-ctx`.

**⚠️ CRITICAL: Environment Isolation**
- **Virtual Envs:** Each directory MUST have its own local `.venv`.
- **Temp Directories:** Explicitly set `GEMINI_TMP_DIR` for each session to prevent cross-worktree cache pollution.

### 5.2 Directory Structure (Visualized)
```text
~/Git/
├── coretext/                  # Main Project & Tool Source
│   ├── coretext/              # Source code
│   ├── experiments/
│   │   ├── specs/             # FROZEN Specs (Phase 1)
│   │   └── results/           # Unified Data Collection
│   └── docs/                  # Methodology & Reports
├── coretext-exp-01-human/     # Isolated Worktree 1
├── coretext-exp-02-std/       # Isolated Worktree 2
└── coretext-exp-03-ctx/       # Isolated Worktree 3
```

### 5.3 Model Selection
**Selected Model:** `gemini-3-pro` (or highest available reasoning tier).
**Rationale:** The experiment measures *Retrieval Efficiency*, not *Coding Capability*. High-reasoning models reduce noise from hallucinations/logic errors, isolating the impact of context retrieval methods. "Flash" models are rejected to minimize "Correction Loops" that skew time/token metrics.

### 5.4 Agent Configuration
- **Subject B Agent:** Uses standard `bmad-developer` instructions. Access to `read_file`.
- **Subject C Agent:** Uses modified `coretext-developer` instructions.
    - **Modification:** `read_file` permission REMOVED for `experiments/specs/` path.
    - **Modification:** `coretext-retrieve` (or `query_knowledge`) tool ADDED.
    - **Prompt:** "Do not read Markdown files. Query the Knowledge Graph for specifications."

### 5.5 Data Logging
At the end of each session, the following must be captured in `experiments/results/run-X/`:
1. **`session.json`**: Full chat history export.
2. **`stats.txt`**: The CLI "Interaction Summary" block containing:
    - Wall Time / Active Time.
    - Input/Output Token counts (Cache hits are noted but secondary).
    - Tool Call counts.
    - Model Usage breakdown.

