# CoreText Evaluation Methodology
**Version:** 1.0
**Date:** 2026-01-27
**Objective:** Comparative analysis of Graph-Based Context Retrieval (CoreText) vs. File-Based Retrieval in AI-driven software generation.

## 1. Experimental Overview
This study evaluates whether CoreText's knowledge-graph retrieval method reduces token consumption and latency while maintaining code quality compared to standard file-based context retrieval methods.

### 1.1 The Task (Workload)
**Subject:** Rental House Listing Platform.
**Complexity Requirements:**
To ensure sufficient complexity for valid differentiation, the specification must include:
- **User Roles:** Multi-tenant architecture (Admin, Landlord, Tenant).
- **Data Relationships:** Complex 1:N and N:N relationships (Listings, Reviews, Bookings, Users).
- **Features:** Search/Filtering, Auth/RBAC, Booking State Machine.
- **Volume:** Specification documents (PRD, Architecture, User Stories) must exceed 10,000 tokens total to stress-test context windows.

---

## 2. Experimental Subjects

### Subject A: Human Baseline (Control 1)
- **Operator:** Senior Developer (Human).
- **Tools:** Standard IDE + AI Coding Assistant (e.g., Copilot/Chat interface).
- **Constraint:** Operator builds the software manually based on BMM specs.
- **Goal:** Establish the "Gold Standard" for logic and execution time.

### Subject B: Standard BMad Agent (Control 2)
- **Agent:** Standard `bmad-developer` agent.
- **Retrieval Method:** **File-Based**. Agent reads full Markdown files (PRD.md, Architecture.md) into context.
- **Workflow:** Standard ReAct loop (Read File -> Plan -> Write Code).
- **Constraint:** Human intervention limited to "Orchestration Only" (running commands, providing error logs).

### Subject C: CoreText Integrated Agent (Test)
- **Agent:** Modified `bmad-developer` agent.
- **Retrieval Method:** **Graph-Based**. Agent uses `coretext-retrieve` (or `query_knowledge`) tool to query specific subgraphs relevant to the current task.
- **Workflow:** Graph Query -> Bundle Context -> Write Code.
- **Constraint:** **Strict Knowledge Isolation**. The agent must NOT have access to the raw Markdown specification files; it must rely solely on the Knowledge Graph.

---

## 3. Metrics & Measurement

### 3.1 Efficiency Metrics (Quantitative)
| Metric | Unit | Definition |
| :--- | :--- | :--- |
| **Input Tokens** | Count | Total tokens sent TO the LLM (Context). **Primary Metric.** |
| **Output Tokens** | Count | Total tokens generated BY the LLM (Code/Reasoning). |
| **API Cost** | USD | Calculated cost based on model pricing. |
| **Context Utility**| Ratio | `(Lines of Code Generated) / (Input Tokens)`. Higher is better. |

### 3.2 Performance Metrics (Temporal)
| Metric | Unit | Definition |
| :--- | :--- | :--- |
| **Total Wall Time** | Minutes | Start to Finish (Definition of Done). |
| **AI Compute Time**| Minutes | Time spent waiting for LLM responses. |
| **Human Action Time**| Minutes| Time spent by human reading, thinking, or typing. |
| **Intervention Count**| Count | Number of times the AI got stuck and needed human guidance. |

### 3.3 Quality Scorecard (0-100 Scale)
Post-build evaluation of the final artifact.
1. **Functional Completeness (40 pts):** % of User Stories in Spec marked "Verified".
2. **Code Health (20 pts):** `100 - (Linter Errors + Warnings)` using a pre-defined config (e.g., Ruff/ESLint).
3. **Test Coverage (20 pts):** % of lines covered by generated tests.
4. **Bug Resilience (20 pts):** Result of 10-minute "Monkey Test" (random inputs).

---

## 4. Execution Protocol

### Phase 1: Specification Generation (Frozen)
1. Run `bmm` (Business Model Module) to generate the Rental Platform Plan.
2. **FREEZE** the output files (PRD, Tech Spec, Stories).
3. These exact files are the input for ALL three experiments.

### Phase 2: Experiment Execution (Run 3 Times)
**Pre-Flight Checklist (The "Clean Room" Rule):**
- [ ] Wipe target directory.
- [ ] Clear LLM conversation history.
- [ ] Reset Database/Docker containers.
- [ ] Verify Model Version (e.g., `gpt-4o-2024-05-13`) is identical for all runs.
- [ ] **Subject C Only:** Ingest Phase 1 specs into CoreText DB and disable `read_file` for spec path.

**Operator Rules of Engagement:**
- **For Subject A (Human):** Code naturally.
- **For Subject B & C (AI):** Do NOT write code. Only execute commands requested by the agent. If an error occurs, paste the error exactly. Log every intervention.

### Phase 3: Analysis
- Compare **Subject C** vs. **Subject B** to validate the efficiency of Graph Retrieval.
- Compare both against **Subject A** to assess the "Reality Gap" between agentic and human development.

## 5. Operational Setup

### 5.1 Repository & Branching Strategy
The experiment utilizes a single git repository with a "Multiverse" branching model to isolate experimental runs while maintaining a common history.

**Branch Structure:**
1. **`main`**: Contains the CoreText source code, `experiments/specs/` (Frozen Phase 1 output), and documentation.
2. **`experiment/baseline-human`**: Forked from `main`. Subject A builds the artifact here.
3. **`experiment/subject-b-standard`**: Forked from `main`. Subject B builds the artifact here.
4. **`experiment/subject-c-coretext`**: Forked from `main`. Subject C builds the artifact here.

**Workflow:**
- Fork experimental branch from `main`.
- Perform the run.
- Commit the final artifact (source code of the Rental App).
- Merge *only* the `experiments/results/` folder back to `main` (if needed) or keep artifacts in branch.
- Use `git diff experiment/subject-b-standard experiment/subject-c-coretext` for code quality comparison.

### 5.2 Directory Structure
```text
/
├── _bmad/ ...              # Standard Agent configs
├── coretext/ ...           # CoreText tool source
├── experiments/
│   ├── specs/              # FROZEN Phase 1 Output (PRD.md, Stories.md)
│   ├── agents/             # Experimental Agent Configs
│   │   ├── subject-b.csv   # Manifest for Standard Agent
│   │   └── subject-c.csv   # Manifest for CoreText Agent
│   └── results/            # Data Collection
│       ├── run-1-human/    # Chat logs, stats.txt, time logs
│       ├── run-2-standard/
│       └── run-3-coretext/
```

### 5.3 Model Selection
**Selected Model:** `gemini-3-pro` (or highest available reasoning tier).
**Rationale:** The experiment measures *Retrieval Efficiency*, not *Coding Capability*. High-reasoning models reduce noise from hallucinations/logic errors, isolating the impact of context retrieval methods. "Flash" models are rejected to minimize "Correction Loops" that skew time/token metrics.

### 5.4 Agent Configuration
- **Subject B Agent:** Uses standard `bmad-developer` instructions. Access to `read_file`.
- **Subject C Agent:** Uses modified `coretext-developer` instructions.
    - **Modification:** `read_file` permission REMOVED for `experiments/specs/` path.
    - **Modification:** `coretext-retrieve` (or `query_knowledge`) tool ADDED.
    - **Prompt:** "Do not read Markdown files. Query the Knowledge Graph for specifications."

### 5.5 Data Logging
At the end of each session, the following must be captured in `experiments/results/run-X/`:
1. **`session.json`**: Full chat history export.
2. **`stats.txt`**: The CLI "Interaction Summary" block containing:
    - Wall Time / Active Time.
    - Input/Output Token counts (Cache hits are noted but secondary).
    - Tool Call counts.
    - Model Usage breakdown.

