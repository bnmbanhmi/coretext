# CoreText Evaluation Methodology
**Version:** 1.4
**Date:** 2026-02-01
**Objective:** Comparative analysis of Graph-Based Context Retrieval (CoreText) vs. File-Based Retrieval in AI-driven software generation.

## 1. Experimental Overview
This study evaluates whether CoreText's knowledge-graph retrieval method reduces token consumption and latency while maintaining code quality compared to standard file-based context retrieval methods.

### 1.1 The Task (Workload)
**Subject:** `trore` (Rental House Listing Platform).
**Specs Location:** `experiments/trore/_bmad-output/`
**Target Implementation:** `experiments/trore/`

**Complexity Requirements:**
The project is a hybrid multi-cloud SPA (React 19 + FastAPI + GCP Refinery).
- **User Roles:** Admin, Seeker, Landlord.
- **Architecture:** Monorepo (Turborepo) with Web, API, and Scraper packages.
- **Data:** Supabase (PostgreSQL) with SCD Type 2 history.
- **Volume:** The `_bmad-output` folder contains extensive PRD, Architecture, and UX specifications.

---

## 2. Experimental Subjects

### Subject A: Human Baseline (Control 1)
- **Operator:** Senior Developer (Human).
- **Tools:** Standard IDE + AI Coding Assistant.
- **Goal:** Establish the "Gold Standard" for logic and execution time.

### Subject B: Standard BMad Agent (Control 2)
- **Workflow:** Standard `create-story` and `dev-story`.
- **Retrieval Method:** **File-Based**. Agent reads full Markdown files from `experiments/trore/_bmad-output/planning-artifacts/` into context.
- **Workflow:** Standard ReAct loop (Read File -> Plan -> Write Code).

### Subject C: CoreText Integrated Agent (Test)
- **Workflow:** Modified `create-story` and `coretext-dev-story`, including modified files:
    -  _bmad/bmm/workflows/4-implementation/dev-story
    -  _bmad/bmm/workflows/4-implementation/create-story
    - _bmad/bmm/config.yaml
    - .gemini/commands/bmad-workflow-bmm-dev-story.toml
    - .gemini/commands/bmad-workflow-bmm-create-story.toml
    - _bmad/core/tasks/workflow.xml
- **Retrieval Method:** **Graph-Based**. Agent uses `coretext` MCP tools (`query_knowledge`, `get_dependencies`) to query specific requirements.
- **Constraint:** **Strict Knowledge Isolation**.
    - **Blocked:** Access to `experiments/trore/_bmad-output/planning-artifacts/**` is blocked via `.geminiignore`.
    - **Allowed:** `experiments/trore/_bmad-output/implementation-artifacts/**` (to track sprint/story status).
    - **Mandate:** Must use CoreText to retrieve architectural patterns, data models, and business rules defined in the hidden planning artifacts.

---

## 3. Metrics & Measurement

### 3.1 Efficiency Metrics (Quantitative)
| Metric              | Unit  | Definition                                                      |
| :------------------ | :---- | :-------------------------------------------------------------- |
| **Input Tokens**    | Count | Total tokens sent TO the LLM (Context). **Primary Metric.**     |
| **Output Tokens**   | Count | Total tokens generated BY the LLM (Code/Reasoning).             |
| **Context Utility** | Ratio | `(Lines of Code Generated) / (Input Tokens)`. Higher is better. |

### 3.2 Performance Metrics (Temporal)
| Metric                 | Unit    | Definition                                                  |
| :--------------------- | :------ | :---------------------------------------------------------- |
| **Total Wall Time**    | Minutes | Start to Finish (Definition of Done).                       |
| **AI Compute Time**    | Minutes | Time spent waiting for LLM responses.                       |
| **Human Action Time**  | Minutes | Time spent by human reading, thinking, or typing.           |
| **Intervention Count** | Count   | Number of times the AI got stuck and needed human guidance. |

### 3.3 Quality Scorecard (0-100 Scale)
Post-build evaluation of the final artifact.
1. **Functional Completeness (40 pts):** % of User Stories in Spec marked "Verified".
2. **Code Health (20 pts):** `100 - (Linter Errors + Warnings)` using a pre-defined config (e.g., Ruff/ESLint).
3. **Test Coverage (20 pts):** % of lines covered by generated tests.
4. **Bug Resilience (20 pts):** Result of 10-minute "Monkey Test" (random inputs).

---

## 4. Execution Protocol

### Phase 1: Specification Ingestion
1. **Source:** `experiments/trore/_bmad-output/`.
2. **Indexing:** Run `coretext sync` targeting this directory to populate the Knowledge Graph.
3. **Verification:** Ensure `coretext inspect` can retrieve nodes.

### Phase 2: Experiment Execution (Run 3 Times)
**Pre-Flight Checklist:**
- [ ] Reset target directories (`apps`, `packages`, `pnpm-lock.yaml`).
- [ ] Clear LLM conversation history.
- [ ] **Subject C Only:** Verify CoreText daemon is running and healthy.
- [ ] **Subject C Only:** Verify `.geminiignore` is active in the worktree.

**Workflow Enforcement:**
- Use the `create-story` and `dev-story` workflows.
- **Subject B:** Prompt explicitly points to `planning-artifacts/prd.md` etc.
- **Subject C:** Prompt explicitly forbids reading `planning-artifacts` and instructs to use `query_knowledge` for "How do I implement X?" or "What is the schema for Y?".

### Phase 3: Analysis
- Compare **Subject C** vs. **Subject B** vs. **Subject A** metrics.

## 5. Operational Setup

### 5.1 Project Directory Structure
This structure represents the internal layout of the `coretext` repository. Each experimental subject runs in its own isolated Worktree of this repo.

```text
~/Git/coretext/                  # Main Repository (Root)
├── coretext/                    # The Tool Source Code (Agent's Brain)
├── experiments/                 # Experimental Data
│   ├── results/                 # Unified Data Collection (evaluation_log.csv)
│   └── trore/             # The Workstation (Specific to each Worktree)
│       ├── _bmad-output/        # The Specs (Source of Truth)
│       │   ├── planning-artifacts/        # READ-DENIED for Subject C via .geminiignore
│       │   └── implementation-artifacts/  # READ-ALLOWED (Status tracking)
│       ├── apps/                # Generated Code (Web/API) - Reset per run
│       └── packages/            # Generated Code (Importer) - Reset per run
└── docs/                        # Methodology & Reports
```

**Experimental Notes:**
*   **Subject A (Human):** Works freely within `apps/` and `packages/` using IDE.
*   **Subject B (Control AI):** Agent reads files from `_bmad-output/planning-artifacts/` to generate code in `apps/`.
*   **Subject C (Test AI):** Agent is BLOCKED from `_bmad-output/planning-artifacts/` but queries CoreText to generate code in `apps/`.

### 5.2 Worktree Isolation Strategy
To prevent contamination, we use **Git Worktrees** to create sibling directories for each subject. All worktrees share the same `.git` history but have independent working directories.

**Sibling Worktree Locations:**
1.  **`../exp-a-human`**: Baseline environment.
2.  **`../exp-b-control`**: Standard BMad Agent environment.
3.  **`../exp-c-coretext`**: CoreText Agent environment (contains `.geminiignore`).

**Initialization Commands:**
```bash
cd ~/Git/coretext

# Create experimental branches
git branch experiment/human main
git branch experiment/control main
git branch experiment/coretext main

# Create Worktrees as siblings
git worktree add ../exp-a-human experiment/human
git worktree add ../exp-b-control experiment/control
git worktree add ../exp-c-coretext experiment/coretext
```

### 5.3 Strict Isolation (The .geminiignore Rule)
To physically prevent Subject C from "cheating" (reading files), a `.geminiignore` file is placed **only** in the Subject C worktree.

**File Location:** `../exp-c-coretext/experiments/trore/.geminiignore`
**Content:**
```text
# BLOCK access to the source of truth (Planning Artifacts)
_bmad-output/planning-artifacts/
```

### 5.4 Data Logging & Export
**Chat History (Subjects B & C):**
- At the end of each story implementation, use the CLI command: `/chat share`
- **Naming Convention:** `exp_[Subject]_[StoryID]_[Date].json`
  - Example: `exp_C_1-1_20260201.json`

**Metrics Log:**
- Results are recorded in `experiments/results/evaluation_log.csv`.

---

## 6. Evaluation Implementation Plan

This checklist guides the facilitator through the full experiment lifecycle.

### 6.1 Preparation Phase
- [ ] **Specs Finalized:** Verify `experiments/trore/_bmad-output/` contains complete PRD, Architecture, and Stories.
- [ ] **CoreText Indexing:**
    - [ ] Run `coretext init` (if needed).
    - [ ] Configure `coretext` to index `experiments/trore/_bmad-output`.
    - [ ] Run `coretext sync`.
- [ ] **Worktrees Created:** Execute the git worktree commands from Section 5.2.
- [ ] **Isolation Applied:** Create the `.geminiignore` file in `exp-c-coretext`.
- [ ] **Log File Created:** Verify `experiments/results/evaluation_log.csv` exists.

### 6.2 Execution Loop (Repeat for each Story 1.1 -> 1.5)

#### Subject A (Control 1: Human Baseline)
1.  **Navigate:** `cd ../exp-a-human`
2.  **Start Timer:** Note start time.
3.  **Execute:** Human developer implements the story using standard IDE tools.
4.  **Finish:** Code committed and verified locally.
5.  **Stop Timer:** Note end time.
6.  **Record Metrics:** Log "Wall Time" and "Human Action Time" in `evaluation_log.csv` (Token metrics = 0/NA).

#### Subject B (Control 2: File-Based AI)
1.  **Navigate:** `cd ../exp-b-control`
2.  **Reset:** Ensure clean state.
3.  **Start Timer:** Note start time.
4.  **Execute:** Run BMad `create-story` and `dev-story` workflow.
    - *Prompt:* "Implement Story X.X. Read `planning-artifacts` for context."
5.  **Finish:** When Agent declares "Story Complete".
6.  **Stop Timer:** Note end time.
7.  **Export:** Run `/chat share`. Rename/Save to `experiments/results/exp_B_[StoryID]_[Date].json`.
8.  **Record Metrics:** Fill out `evaluation_log.csv`.

#### Subject C (Test: CoreText-Based AI)
1.  **Navigate:** `cd ../exp-c-coretext`
2.  **Verify Isolation:** Check `.geminiignore` exists.
3.  **Start Timer:** Note start time.
4.  **Execute:** Run modified BMad `create-story` and `dev-story` workflow.
    - *Prompt:* "Implement Story X.X. Do NOT read planning files. Use `query_knowledge` tool."
5.  **Finish:** When Agent declares "Story Complete".
6.  **Stop Timer:** Note end time.
7.  **Export:** Run `/chat share`. Rename/Save to `experiments/results/exp_C_[StoryID]_[Date].json`.
8.  **Record Metrics:** Fill out `evaluation_log.csv`.

### 6.3 Post-Epic Analysis
- [ ] **Metric Aggregation:** Sum tokens/time per Subject.
- [ ] **Quality Audit:** Run the "Quality Scorecard" (Section 3.3) on the final codebases.
- [ ] **Report Generation:** Create `docs/evaluation-report.md`.